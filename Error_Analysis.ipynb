{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>predictions</th>\n",
       "      <th>references</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the reopening rules for bars and rest...</td>\n",
       "      <td>Bars and restaurants may currently open for dr...</td>\n",
       "      <td>[' The reopening of indoor dining spaces has b...</td>\n",
       "      <td>Bars and restaurants may currently open for dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the reopening rules for bars and rest...</td>\n",
       "      <td>Bars and restaurants may currently open for dr...</td>\n",
       "      <td>[' Restaurants offering service in outdoor are...</td>\n",
       "      <td>Bars and restaurants may currently open for dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the reopening rules for bars and rest...</td>\n",
       "      <td>Bars and restaurants may currently open for dr...</td>\n",
       "      <td>[' Restaurants may offer in-person dining serv...</td>\n",
       "      <td>Bars and restaurants may currently open for dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the required safety protocols or soci...</td>\n",
       "      <td>If feasible and consistent with social distan...</td>\n",
       "      <td>[' Comply with state and CDC guidelines to pro...</td>\n",
       "      <td>Elective surgeries are permitted to resume in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the required safety protocols or soci...</td>\n",
       "      <td>If feasible and consistent with social distan...</td>\n",
       "      <td>[\" Utilizing telemedicine to the greatest exte...</td>\n",
       "      <td>Elective surgeries are permitted to resume in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What are the reopening rules for bars and rest...   \n",
       "1  What are the reopening rules for bars and rest...   \n",
       "2  What are the reopening rules for bars and rest...   \n",
       "3  What are the required safety protocols or soci...   \n",
       "4  What are the required safety protocols or soci...   \n",
       "\n",
       "                                         predictions  \\\n",
       "0  Bars and restaurants may currently open for dr...   \n",
       "1  Bars and restaurants may currently open for dr...   \n",
       "2  Bars and restaurants may currently open for dr...   \n",
       "3   If feasible and consistent with social distan...   \n",
       "4   If feasible and consistent with social distan...   \n",
       "\n",
       "                                          references  \\\n",
       "0  [' The reopening of indoor dining spaces has b...   \n",
       "1  [' Restaurants offering service in outdoor are...   \n",
       "2  [' Restaurants may offer in-person dining serv...   \n",
       "3  [' Comply with state and CDC guidelines to pro...   \n",
       "4  [\" Utilizing telemedicine to the greatest exte...   \n",
       "\n",
       "                                             context  \n",
       "0  Bars and restaurants may currently open for dr...  \n",
       "1  Bars and restaurants may currently open for dr...  \n",
       "2  Bars and restaurants may currently open for dr...  \n",
       "3  Elective surgeries are permitted to resume in ...  \n",
       "4  Elective surgeries are permitted to resume in ...  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "data = pd.read_csv('Prediction_References2.csv', index_col = 0)\n",
    "# data = data.loc[:, ~data.columns.str.contains('^Unnamed')]  #drop unnamed columns\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_brackets(column):\n",
    "    column = re.sub(r'\\[\\'\\s*|\\[\\\"\\s*',\"\", column)  #front brackets\n",
    "    return re.sub(r'\\s*\\'\\]|\\\"\\]',\"\", column) #end brackets\n",
    "\n",
    "\n",
    "def remove_whitespace(column): #remove any trailing whitespace\n",
    "    if type(column) == str:\n",
    "        return column.strip()\n",
    "    \n",
    "    \n",
    "data['references'] = data['references'].apply(remove_brackets)    \n",
    "data['predictions'] = data['predictions'].apply(remove_whitespace)\n",
    "data['predictions'].fillna(0,inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Averages\n",
    "\n",
    "Average Context Token Ct:  462 <br>\n",
    "Average Reference Token Ct:  66 <br>\n",
    "Average Prediction Token Ct:  69 <br>\n",
    "Average Question Token Ct:  10 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Answer Start for Correct Predictions:  244.25\n",
      "\n",
      "Average Answer Start for NO Predictions:  1132.55\n",
      "\n",
      "Average Answer Start for Incorrect Predictions:  666.9736842105264\n",
      "23  Incorrect predictions with answer starting at index 0\n",
      "73 incorrect predictions\n",
      "47 / 73 incorrect predictions had a reference with a higher “answer_start” index than the prediction provided.\n",
      "35 Instances in which the prediction was incorrect and the reference was longer than the prediction\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def pattern_search(pattern,text):\n",
    "    correct_list= []\n",
    "    if re.search(pattern,text):\n",
    "        match = re.search(pattern,text)\n",
    "        answer_start = match.start()\n",
    "        correct_list.append(answer_start)\n",
    "    elif re.search(re.escape(pattern), text):\n",
    "        match = re.search(re.escape(pattern),text)\n",
    "        answer_start = match.start()    #get answer_start\n",
    "        correct_list.append(answer_start)    #append to a list\n",
    "    else:\n",
    "        correct_list.append('NaN1')  #couldn't find a match\n",
    "    return correct_list\n",
    "    \n",
    "\n",
    "def create_answer_starts(question,reference,predictions,context):\n",
    "    correct_predictions = []\n",
    "    no_prediction = []\n",
    "    incorrect_prediction = []\n",
    "    prediction_idx = []\n",
    "    reference_idx = []\n",
    "    for index,row in enumerate(question):\n",
    "        pattern = reference[index]  \n",
    "        text = context[index]  \n",
    "        if predictions[index] == reference[index]:  #if the prediction is the same as the reference(gt), \n",
    "            correct_predictions.extend(pattern_search(pattern,text)) #find where the answer starts in the context\n",
    "        if predictions[index] ==0:\n",
    "            no_prediction.extend(pattern_search(pattern,text))    #find where the answer starts in the context\n",
    "        if predictions[index] != reference[index] and predictions[index] != 0: #if the prediction is neither 0 nor the right one\n",
    "            incorrect_prediction.extend(pattern_search(pattern,text))\n",
    "        if predictions[index] != reference[index]:\n",
    "            pred_pattern = predictions[index]\n",
    "            if type(pred_pattern) == str:\n",
    "                reference_idx.extend(pattern_search(pattern,text))  #create an index of references\n",
    "                prediction_idx.extend(pattern_search(pred_pattern,text)) #create an index of predictions\n",
    "            else:\n",
    "                reference_idx.append('Null')  #create an index of references\n",
    "                prediction_idx.append('Null') #create an index of predictions\n",
    "        else: \n",
    "                reference_idx.append('correct')  #create an index of references\n",
    "                prediction_idx.append('correct') #create an index of predictions\n",
    "    return correct_predictions, no_prediction, incorrect_prediction, prediction_idx, reference_idx\n",
    "\n",
    "def average_answer_start(alist):\n",
    "    num_list = [num for num in alist if type(num) == int]\n",
    "    return sum(num_list) /len(num_list)\n",
    "\n",
    "                \n",
    "\n",
    "correct_prediction, no_prediction, incorrect_prediction,prediction_idx, reference_idx = create_answer_starts(data['question'],data['references'],data['predictions'],data['context'])\n",
    "\n",
    "\n",
    "print(\"Average Answer Start for Correct Predictions: \", average_answer_start(correct_prediction))\n",
    "print()\n",
    "print(\"Average Answer Start for NO Predictions: \", average_answer_start(no_prediction))\n",
    "print()\n",
    "print(\"Average Answer Start for Incorrect Predictions: \", average_answer_start(incorrect_prediction))\n",
    "\n",
    "ct = 0\n",
    "for i in prediction_idx:\n",
    "    if type(i) == int:\n",
    "        ct+=1\n",
    "\n",
    "zipped = list(zip(reference_idx, prediction_idx))\n",
    "ct =0\n",
    "for values in zipped:\n",
    "    if values[1] == 0: ct+=1\n",
    "print(ct, \" Incorrect predictions with answer starting at index 0\")\n",
    "\n",
    "dic = {}\n",
    "dic1 = {}\n",
    "for index, value in enumerate(zipped):\n",
    "    if type(value[0]) == int and type(value[1]) == int:\n",
    "        dic1[index] = value\n",
    "        if value[0] > value[1]:\n",
    "            dic[index] = value\n",
    "\n",
    "print(len(dic1), \"incorrect predictions\") #73 incorrect predictions\n",
    "print(len(dic),\"/\",len(dic1), \"incorrect predictions had a reference with a higher “answer_start” index than the prediction provided.\")\n",
    "\n",
    "dic3 = {}\n",
    "for key in dic1:\n",
    "    if len(data['references'][key]) > len(data['predictions'][key]):\n",
    "        dic3[key]= dic1[key]\n",
    "print(len(dic3),\"Instances in which the prediction was incorrect and the reference was longer than the prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 WHAT - questions\n",
      "11 HOW - questions\n",
      "10 ARE - questions\n",
      "8 IS - questions\n",
      "4 WHERE - questions\n",
      "1 WHEN - questions\n",
      "0 IF - questions\n"
     ]
    }
   ],
   "source": [
    "#rough estimate of what question types we are working with\n",
    "what_lst = []\n",
    "is_lst = []\n",
    "are_lst = []\n",
    "when_lst = []\n",
    "where_lst = []\n",
    "how_lst = []\n",
    "if_lst = []\n",
    "\n",
    "\n",
    "for question in data['question']:\n",
    "    if 'What' in question:\n",
    "        what_lst.append(question)\n",
    "    if 'Is' in question:\n",
    "        is_lst.append(question)\n",
    "    if 'Are' in question:\n",
    "        are_lst.append(question)\n",
    "    if 'When' in question:\n",
    "        when_lst.append(question)\n",
    "    if 'Where' in question:\n",
    "        where_lst.append(question)\n",
    "    if 'How' in question:\n",
    "        how_lst.append(question)\n",
    "    if 'If' in question:\n",
    "        if_lst.append(question)\n",
    "        \n",
    "print(len(what_lst), \"WHAT - questions\")\n",
    "print(len(how_lst), \"HOW - questions\")\n",
    "print(len(are_lst), \"ARE - questions\")\n",
    "print(len(is_lst), \"IS - questions\")\n",
    "print(len(where_lst), \"WHERE - questions\")\n",
    "print(len(when_lst), \"WHEN - questions\")\n",
    "print(len(if_lst), \"IF - questions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Context Token Ct:  462\n",
      "Average Question Token Ct:  10\n",
      "Average Reference Token Ct:  66\n",
      "Average Prediction Token Ct:  69\n"
     ]
    }
   ],
   "source": [
    "context_length = []\n",
    "question_length = []\n",
    "references_length = []\n",
    "predictions_length = []\n",
    "\n",
    "\n",
    "def average_tokens(series, alist):\n",
    "    for string in series:\n",
    "        if type(string) == str:\n",
    "            tkns = word_tokenize(string)\n",
    "            alist.append(len(tkns))\n",
    "    average = (sum(alist) / len(alist))\n",
    "\n",
    "    return round(average)\n",
    "\n",
    "print(\"Average Context Token Ct: \", average_tokens(data.context, context_length))\n",
    "print(\"Average Question Token Ct: \", average_tokens(data.question, question_length))\n",
    "print(\"Average Reference Token Ct: \", average_tokens(data.references, references_length))\n",
    "print(\"Average Prediction Token Ct: \", average_tokens(data.predictions, predictions_length))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 119 incorrect predictions\n",
    " \n",
    "Number of Incorrect Predictions 119 <br>\n",
    "Average Token Ct of Context 476 <br>\n",
    "Average Token Ct of References 73 <br>\n",
    "Average Token Ct of Questions 10 <br>\n",
    "Number of Incorrect Predictions with context > 1024 = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Incorrect Predictions 119\n",
      "Average Token Ct of Context 476\n",
      "Average Token Ct of References 73\n",
      "Average Token Ct of Questions 10\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk import word_tokenize\n",
    "\n",
    "def average_tokens(string, alist):\n",
    "    tkns = word_tokenize(string)\n",
    "    alist.append(len(tkns))\n",
    "    average = (sum(alist) / len(alist))\n",
    "    return len(tkns), average\n",
    "\n",
    "\n",
    "ct = 0  # of incorrect predictions\n",
    "avg_context = []\n",
    "avg_reference = []\n",
    "avg_question = []\n",
    "data_length = len(data)\n",
    "for row in range(data_length):\n",
    "    if data['predictions'][row] != data['references'][row]:\n",
    "        ct+=1\n",
    "#         print(data['question'][row])\n",
    "        context_tokens, avg_cntx = average_tokens(data.loc[row,'context'], avg_context)\n",
    "        reference_tokens, avg_ref = average_tokens(data.loc[row,'references'], avg_reference)\n",
    "        question_tokens, avg_quest = average_tokens(data.loc[row,'question'], avg_question)\n",
    "        \n",
    "#         print(\"Length of Context:\", (context_tokens))\n",
    "#         print(\"Length of Reference (correct answer):\",(reference_tokens))\n",
    "#         if len(context_tokens) > 1024:\n",
    "#             print(\"Context Longer than 1024\")\n",
    "        \n",
    "#     print(\" ##############\")\n",
    "    \n",
    "print(\"Number of Incorrect Predictions\",ct)\n",
    "print(\"Average Token Ct of Context\", round(avg_cntx))\n",
    "print(\"Average Token Ct of References\",round(avg_ref))\n",
    "print(\"Average Token Ct of Questions\",round(avg_quest))\n",
    "\n",
    "                                    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40 Null Predictions\n",
    "\n",
    "Average Token Ct of Context: 460  (only 25 contexts here were above average token ct) <br>\n",
    "Average Token Ct of References: 73 <br>\n",
    "Average Token Ct of Questions: 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Null Answers 40\n",
      "Average Token Ct of Context 460\n",
      "Average Token Ct of References 62\n",
      "Average Token Ct of Questions 8\n",
      "Only  25 contexts were above average length\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ct = 0  # of NULL Answers\n",
    "\n",
    "avg_context = []\n",
    "avg_references = []\n",
    "avg_question = []\n",
    "\n",
    "for row in range(data_length):\n",
    "    if data['predictions'][row] == 0:\n",
    "        ct+=1\n",
    "        context_tokens, avg_cntx = average_tokens(data.loc[row,'context'], avg_context)\n",
    "        reference_tokens, avg_ref = average_tokens(data.loc[row,'references'], avg_reference)\n",
    "        question_tokens, avg_quest = average_tokens(data.loc[row,'question'], avg_question)\n",
    "\n",
    "#         print(data['question'][row])\n",
    "#         print(\"Length of Context:\",(context_tokens))\n",
    "#         print(\"Length of Reference (correct answer):\",(reference_tokens))\n",
    "# #         if len(context_tokens) > 1024:\n",
    "# #             print(\"Context Longer than 1024\")\n",
    "#         print(\" ##############\")\n",
    "    \n",
    "    \n",
    "print(\"Number of Null Answers\", ct)\n",
    "print(\"Average Token Ct of Context\", round(avg_cntx))\n",
    "print(\"Average Token Ct of References\",round(avg_ref))\n",
    "print(\"Average Token Ct of Questions\",round(avg_quest))\n",
    "\n",
    "\n",
    "ct=0\n",
    "for tk_count in avg_context:\n",
    "    if tk_count >= avg_cntx:\n",
    "        ct+=1\n",
    "\n",
    "print(\"Only \", ct, \"contexts were above average length\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 20 Correct Responses\n",
    "\n",
    "Average Token Ct of Context: 377 <br>\n",
    "Average Token Ct of References: 24 <br>\n",
    "Average Token Ct of Questions: 11 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Longer than 1024\n",
      "Number of Correct Answers 20\n",
      "Average Token Ct of Context 377\n",
      "Average Token Ct of References 58\n",
      "Average Token Ct of Questions 11\n"
     ]
    }
   ],
   "source": [
    "ct = 0\n",
    "\n",
    "avg_context = []\n",
    "avg_references = []\n",
    "avg_question = []\n",
    "\n",
    "for row in range(data_length):\n",
    "    if data['predictions'][row] == data['references'][row]:\n",
    "        ct+=1\n",
    "#         print(data['question'][row])\n",
    "#         print(\"prediction: \", data['predictions'][row], \"reference: \",data['references'][row])\n",
    "        context_tokens = word_tokenize(data['context'][row])\n",
    "#         print(len(context_tokens))\n",
    "        if len(context_tokens) > 1024:\n",
    "            print(\"Context Longer than 1024\")\n",
    "        context_tokens, avg_cntx = average_tokens(data.loc[row,'context'], avg_context)\n",
    "        reference_tokens, avg_ref = average_tokens(data.loc[row,'references'], avg_reference)\n",
    "        question_tokens, avg_quest = average_tokens(data.loc[row,'question'], avg_question)\n",
    "\n",
    "#         print(\" ##############\")\n",
    "\n",
    "\n",
    "    \n",
    "print(\"Number of Correct Answers\", ct)\n",
    "print(\"Average Token Ct of Context\", round(avg_cntx))\n",
    "print(\"Average Token Ct of References\",round(avg_ref))\n",
    "print(\"Average Token Ct of Questions\",round(avg_quest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Yes/No questions in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "ct = 0\n",
    "for reference in data.references:\n",
    "    if reference == \"Yes\" or reference == \"No\":\n",
    "        ct+=1\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Yes/No predictions in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "ct = 0\n",
    "for prediction in data.predictions:\n",
    "    if prediction  == \"Yes\" or prediction == \"No\":\n",
    "        ct+=1\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 58 Duplicate Questions (with different answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are: 58  Instances of Repeating Questions\n"
     ]
    }
   ],
   "source": [
    "duplicateRowsDF = data['question'][data['question'].duplicated()]\n",
    "drl = len(duplicateRowsDF)\n",
    "\n",
    "print(\"There are:\", drl ,\" Instances of Repeating Questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz \n",
    "\n",
    "ratio_list = []\n",
    "for row in range(len(data)):\n",
    "    if data['predictions'][row] != 0:\n",
    "        str1 = data['predictions'][row]\n",
    "        str2 = data['references'][row]\n",
    "        Ratio = fuzz.ratio(str1.lower(),str2.lower())\n",
    "        ratio_list.append(Ratio)\n",
    "    else:\n",
    "        ratio_list.append('NaN')\n",
    "        \n",
    "ct = 0\n",
    "for index, item in enumerate(ratio_list):\n",
    "    if item != 'NaN' and item >= 90:       #take all the predictions that are 90 or higher\n",
    "#         print(data['predictions'][index])\n",
    "#         print()\n",
    "#         print(data['references'][index])\n",
    "#         print('@@@@@@@@@@@@@@@@@@@@@@@@')\n",
    "        ct+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Fuzzy Matching there are 24 correct predictions\n"
     ]
    }
   ],
   "source": [
    "print(\"With Fuzzy Matching there are\", ct, \"correct predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
